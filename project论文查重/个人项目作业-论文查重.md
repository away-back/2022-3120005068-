# [个人项目作业-论文查重](https://github.com/away-back/3120005068/tree/master/project%E8%AE%BA%E6%96%87%E6%9F%A5%E9%87%8D)

[TOC]

###  作业基本信息

| 2022秋软件工程 | https://bbs.csdn.net/topics/607953192                        |
| -------------- | ------------------------------------------------------------ |
| 作业要求       | https://bbs.csdn.net/topics/608092799                        |
| 作业的目标     | 设计一个论文查重算法，给出一个原文文件和一个在这份原文上经过了增删改的抄袭版论文的文件，在答案文件中输出其重复率。 |





### PSP表格

| ***\*PSP2.1\****                        | ***\*Personal Software Process Stages\**** | ***\*预估耗时（分钟）\**** | ***\*实际耗时（分钟）\**** |
| --------------------------------------- | ------------------------------------------ | -------------------------- | -------------------------- |
| Planning                                | 计划                                       | 30                         | 60                         |
| · Estimate                              | · 估计这个任务需要多少时间                 | 600                        | 800                        |
| Development                             | 开发                                       | 240                        | 360                        |
| · Analysis                              | · 需求分析 (包括学习新技术)                | 120                        | 180                        |
| · Design Spec                           | · 生成设计文档                             | 30                         | 30                         |
| · Design Review                         | · 设计复审                                 | 60                         | 60                         |
| · Coding Standard                       | · 代码规范 (为目前的开发制定合适的规范)    | 30                         | 30                         |
| · Design                                | · 具体设计                                 | 60                         | 90                         |
| · Coding                                | · 具体编码                                 | 120                        | 150                        |
| · Code Review                           | · 代码复审                                 | 10                         | 10                         |
| · Test                                  | · 测试（自我测试，修改代码，提交修改）     | 30                         | 240                        |
| Reporting                               | 报告                                       | 20                         | 30                         |
| · Test Repor                            | · 测试报告                                 | 30                         | 30                         |
| · Size Measurement                      | · 计算工作量                               | 30                         | 30                         |
| · Postmortem & Process Improvement Plan | · 事后总结, 并提出过程改进计划             | 30                         | 60                         |
|                                         | · 合计                                     |                            |                            |





### ***计算模块接口的设计与实现过程***

![image-20220923125548701](%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE%E4%BD%9C%E4%B8%9A-%E8%AE%BA%E6%96%87%E6%9F%A5%E9%87%8D.assets/image-20220923125548701.png)





**TF-IDF的主要思想是**：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类



 **<font color='red'>词频（TF）</font>**表示词条（关键字）在文本中出现的频率。
$$
TF(d,w)=N(w)/N(d)
$$
其中N(w)表示**d**这篇文章中词**w**的总数，N(d)表示文档d中的**词的总数**。	


**<font color='red'>逆向文件频率 (IDF)</font>** ：逆文档频率，指的是含有这个单词的文章占语料库中所有文章总数的比例的倒数。**可由总文件数目除以包含该词语的文件的数目**，**再将得到的商取对数得到**
$$
IDF(w)=log(N/N(w))
$$
其中，N 指的是语料库中所有文档的总数，N(w)指的是语料库中含有词w的文档的总数。


<font color='red'>**TF-IDF**</font>是**TF**乘以**IDF**：
$$
TF-IDF(w)=TF(d,w)*IDF(w)
$$




### 计算模块接口部分的性能改进

![image-20220923203906630](%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE%E4%BD%9C%E4%B8%9A-%E8%AE%BA%E6%96%87%E6%9F%A5%E9%87%8D.assets/image-20220923203906630.png)

报告结果：

1. 整个过程一共有4个函数调用被监控
2. 总共执行的时间为0.016秒





### 

```python
arg = sys.argv  # 读取参数(文件目录)
    f = open(arg[1], 'r', encoding='utf-8')
    f2 = open(arg[2], 'r', encoding='utf-8')
    lines = f.read() #读文件
    lines_del = drop_punctuation(lines)
    lines_sep = Separatesentence([lines_del])                       # 标准文件分词数组
    lines_sep.append(['占位'])                                       #列表添加列表元素，使得lines_sep元素都是列表，将为一维
    line2 =jieba.lcut(drop_punctuation(f2.read()))                  # 查重文件分词数组
    f.close()
    f2.close()
    print("标准文件", lines_sep)
    print("标准文件维度"f'{(np.array(lines_sep)).shape}')
    print('要查重的文件', line2)
    print("line2维度"f'{(np.array(line2)).shape}')
    dictionary = corpora.Dictionary(lines_sep)                      # 唯一词典
    num_features = len(dictionary.token2id)                         # dictionary.token2id   为词语打上标签
    print('词典：', dictionary.token2id)
    corpus = [dictionary.doc2bow(text)for text in lines_sep]        # dictionary.doc2bow(text) 统计每个词语重复的次数
    print('语料库：', corpus)
    print("corpus向量", corpus)
    print("字库向量维度"f'{(np.array(corpus)).shape}')
    # corpus = dictionary.doc2bow(lines_sep)
    # new_vec = dictionary.doc2bow(line2)
    new_vec = [dictionary.doc2bow(text) for text in [line2]]
    print("查重文件new_vec向量", new_vec[0])                          #new_vec向量为三维，取首二维元素
    print("new_vec向量维度"f'{(np.array(new_vec[0])).shape}')
    # corpu = np.array(new_vec)
    # print(f'{corpu.shape}')
    print('====================================================')
    tfidf = models.TfidfModel(corpus, dictionary=dictionary)        #构建TF-IDF模型，用corpus来训练模型
    corpus_tfidf = tfidf[corpus]
    test_vec_tfidf = tfidf[new_vec[0]]
    index = similarities.SparseMatrixSimilarity(corpus_tfidf, num_features=len(dictionary.keys()))
    print('\nTF-IDF模型的稀疏向量集：')
    for a in corpus_tfidf:
        print(a)
    print('\nTF-IDF模型的查重文件稀疏向量：')
    print(test_vec_tfidf)
    print('\n相似度计算：')
    sim = index[test_vec_tfidf]
    print(sim[0])
    np.savetxt(arg[3]+'\结果.txt',sim)                    #查重结果保存本地

# Press the green button in the gutter to run the script.
if __name__ == '__main__':
    main()
    #profile.run('main()')
```

 ![image-20220923204651807](%E4%B8%AA%E4%BA%BA%E9%A1%B9%E7%9B%AE%E4%BD%9C%E4%B8%9A-%E8%AE%BA%E6%96%87%E6%9F%A5%E9%87%8D.assets/image-20220923204651807.png)